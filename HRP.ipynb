{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMK/n6dYvh6zuqYNH76e37e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd\n"],"metadata":{"id":"Nl_kHBrL1DkD"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1oWP1Ltgf1C"},"outputs":[],"source":["#Construct grid for returns distribution\n","\n","fig, axs = plt.subplots(3, 3, figsize=(15, 15))\n","\n","for i, col in enumerate(returns.columns):\n","    axs[i // 3, i % 3].hist(returns[col].dropna(), bins=50, edgecolor='k')\n","    axs[i // 3, i % 3].set_title(f'Distribution of Returns for {col}')\n","    axs[i // 3, i % 3].set_xlabel('Returns')\n","    axs[i // 3, i % 3].set_ylabel('Frequency')\n","\n","# My dataset only has 7 columns. For neat presentation, I turn off the empty subplots.\n","\n","axs[2, 1].axis('off')\n","axs[2, 2].axis('off')\n","\n","plt.tight_layout()"]},{"cell_type":"code","source":["# Graph total returns distribution\n","# I used roughly the same colour as previous graph to distinguish between variables without changing context.\n","\n","plt.figure(figsize=(10, 6))\n","plt.hist(returns.values.flatten(), bins=50, edgecolor='k', color='steelblue')\n","plt.title('Distribution of Returns', fontsize=14)\n","plt.xlabel('Returns', fontsize=12)\n","plt.ylabel('Frequency', fontsize=12)\n","plt.grid(axis='y', linestyle='--', alpha=0.7)\n","plt.show()\n","\n","# Optional addiitons. I usually print stats with graphs to help with the interpretation.\n","print(\"Mean of Returns:\\n\", returns.mean())\n","print(\"\\nStandard Deviation of Returns:\\n\", returns.std())\n","print(\"\\nSkewness of Returns:\\n\", returns.skew())\n","print(\"\\nKurtosis of Returns:\\n\", returns.kurtosis())"],"metadata":{"id":"hH4cmnVvw-t9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# HRP uses correlation as distance metric. First contruct a correlation matrix,\n","# convert into distance matrix, then use scipy's linkage() to perform the clustering.\n","# here I am using the average linkage method.\n","\n","# Hierarchical clusters are usually visualised using dendrograms.\n","# I needed one to add to my research so I embedded it into the functions.\n","\n","def HRP(returns_data, max_clusters=4, plot_dendrogram=True):\n","\n","    correlation_matrix = np.corrcoef(returns_data.T)\n","    distance_matrix = np.sqrt(0.5 * (1 - correlation_matrix))\n","    linkage_matrix = linkage(distance_matrix, method='average')\n","\n","\n","    if plot_dendrogram:\n","        plt.figure(figsize=(10, 7))\n","        dendrogram(linkage_matrix, labels=returns_data.columns, leaf_rotation=90)\n","        plt.title('Hierarchical Risk Parity Dendrogram')\n","        plt.xlabel('Indices')\n","        plt.ylabel('Distance')\n","        plt.show()\n","\n","    clusters = fcluster(linkage_matrix, t=max_clusters, criterion='maxclust')\n","\n","    return clusters\n","\n","# Optional. Helps with interpretation.\n","print(\"Cluster Assignments:\", clusters)\n","\n","\n","# Important to store the cluster labels.\n","clusters = HRP(returns, max_clusters=4)\n","cluster_mapping = dict(zip(returns.columns, clusters))\n","print(\"Cluster Mapping:\", cluster_mapping)"],"metadata":{"id":"uD3kHiopxyQT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# I verified the clusters using a correlation matrix for my columns (i.e. returns data)\n","# There are many advanced forms to verify. However, in my case, I picked the one that\n","# would instantly reasonate with my audience and fit in with the overall direction of the research.\n","\n","# Best presented in a heatmap.\n","\n","\n","    correlation_matrix =returns.corr()\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(\n","        correlation_matrix,\n","        annot=True,\n","        cmap=\"coolwarm\",\n","        vmin=-1,\n","        vmax=1,\n","        fmt=\".2f\",\n","        linewidths=0.5,\n","        cbar_kws={\"label\": \"Correlation\"},\n","        square=True,\n","    )\n","    plt.title(\"Correlation Between Indexes\", fontsize=14)\n","    plt.show()"],"metadata":{"id":"mqtsSmad0Y-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# I computed each cluster's annualised performance metrics and stored them in a dictionary for later use.\n","\n","\n","def cluster_performance(returns, cluster_mapping, ppy=12):\n","# ppy signifies period per year.\n","    cluster_performance = {}\n","\n","    for cluster_label in set(cluster_mapping.values()):\n","        cluster_assets = [asset for asset, cluster in cluster_mapping.items() if cluster == cluster_label]\n","\n","        cluster_returns = returns[cluster_assets]\n","\n","        cluster_expected_return = (1 + cluster_returns.mean().mean()) ** ppy - 1\n","        cluster_risk = cluster_returns.std().mean() * (ppy ** 0.5)\n","\n","        # Store results in a dictionary\n","        cluster_performance[cluster_label] = {\n","            \"Annualized Expected Return\": cluster_expected_return,\n","            \"Annualized Risk (Volatility)\": cluster_risk,\n","        }\n","\n","    return cluster_performance"],"metadata":{"id":"evtDEKT21Pz7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cluster_performance = cluster_performance(returns, cluster_mapping)\n","for cluster, performance in cluster_performance.items():\n","    print(f\"Cluster {cluster}:\")\n","    print(f\"  Annualized Expected Return: {performance['Annualized Expected Return']:.4%}\")\n","    print(f\"  Annualized Risk (Volatility): {performance['Annualized Risk (Volatility)']:.4%}\")\n","\n","# This aproach to portfolio construction is very manual. The main reason is that I want to construct 3 portfolios with different constraints:\n","# one allocates equal weights, one aims for a maximum sharpe ratio and one has minimum risk.\n","# My dataset has 7 columns and 175 rows. When dealing with larger datasets, it is best to use optimisation tools from PyPortfolioOpt library.\n","\n","risk_free_rate = 2\n","expected_returns = np.array([c[\"Annualized Expected Return\"] for c in cluster_performance.values()])\n","risks = np.array([c[\"Annualized Risk (Volatility)\"] for c in cluster_performance.values()])\n","\n","# Calculate Sharpe Ratios\n","sharpe_ratios = (expected_returns - risk_free_rate) / risks\n","\n","# Equal weights\n","equal_weights = np.full(len(clusters), 1 / len(clusters))\n","equal_return = np.dot(equal_weights, expected_returns)\n","equal_risk = np.sqrt(np.dot(equal_weights**2, risks**2))\n","equal_sharpe = (equal_return - risk_free_rate) / equal_risk\n","\n","# Maximum Sharpe ratio\n","max_sharpe_weights = sharpe_ratios / sharpe_ratios.sum()\n","max_sharpe_return = np.dot(max_sharpe_weights, expected_returns)\n","max_sharpe_risk = np.sqrt(np.dot(max_sharpe_weights**2, risks**2))\n","max_sharpe_sharpe = (max_sharpe_return - risk_free_rate) / max_sharpe_risk\n","\n","# Minimum risk\n","min_risk_weights = 1 / risks\n","min_risk_weights /= min_risk_weights.sum()\n","min_risk_return = np.dot(min_risk_weights, expected_returns)\n","min_risk_risk = np.sqrt(np.dot(min_risk_weights**2, risks**2))\n","min_risk_sharpe = (min_risk_return - risk_free_rate) / min_risk_risk\n","\n","\n","\n","# Assemble in a df. Not the ideal approach for neat presetantion, but served me well for my research.\n","# If you need a presentable table, you could format it and save to xlsx.\n","\n","portfolios = pd.DataFrame({\n","    \"Portfolio\": [\"Equal Weights\", \"Max Sharpe Ratio\", \"Min Risk\"],\n","    \"Weights\": [equal_weights, max_sharpe_weights, min_risk_weights],\n","    \"Expected Return\": [equal_return, max_sharpe_return, min_risk_return],\n","    \"Risk (Volatility)\": [equal_risk, max_sharpe_risk, min_risk_risk],\n","    \"Sharpe Ratio\": [equal_sharpe, max_sharpe_sharpe, min_risk_sharpe],\n","})\n","\n","portfolios"],"metadata":{"id":"_Q4i_t962gd7"},"execution_count":null,"outputs":[]}]}